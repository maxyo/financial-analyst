# Описание проекта

Цель проекта: помочь анализировать финансовые рынки, автоматически собирая разнородные данные (котировки, отчеты, данные о сделках/инсайдерских операциях, новости/публикации) из надежных источников с помощью скраперов и преобразуя их в структурированные инсайты с помощью ИИ.

Проект состоит из двух частей:
- Backend (NestJS/Express, TypeScript, BullMQ, SQLite): отвечает за сбор данных скраперами, нормализацию/хранение, постановку задач в очереди, ИИ‑обработку и выдачу результатов через API.
- Frontend (React + esbuild): минимальный интерфейс и вспомогательные клиенты для работы с API, а также интеграция с типами, сгенерированными из OpenAPI.

Как это работает (конвейер данных):
1) Источники данных: котировки (биржи/агрегаторы), корпоративные отчеты (ежеквартальные/годовые, раскрытия), сделки (инсайдерские, крупные блок‑трейды), новости (официальные ленты, СМИ, блоги), а также произвольные документы.
2) Скраперы собирают данные, приводят их к минимально необходимому формату и отправляют в хранилище (SQLite через better-sqlite3). Для каждого источника может быть своя стратегия парсинга.
3) Нормализация и обогащение: документы и записи унифицируются (метаданные, время, эмитент/тикер, тип события), связываются с профилями анализа и источниками.
4) ИИ‑обработка: LLM‑сервис агрегирует пачку документов по профилю/периоду и формирует итоговые выводы (summary, сигналы, риски, драйверы). Есть фоновые воркеры (BullMQ) для очередей: например, ai.aggregate-analysis и ai.summarize-documents.
5) Отчеты и API: результаты анализа (reports) доступны через REST‑контроллеры; фронтенд и внешние системы получают их через согласованный OpenAPI‑клиент.

Ключевые сущности и модули:
- Источник документа (DocumentSource): откуда поступают данные (URL, тип — котировки/новости/отчеты/сделки, расписание, правила парсинга).
- Документ (Document): сырые и/или нормализованные записи с метаданными (время, эмитент, категория, оригинальная ссылка, язык).
- Профиль анализа (AnalysisProfile): правила агрегации по тикеру/сектору/рынку, период, предпочитаемые источники, параметры ИИ.
- Отчет (Report): агрегированный результат ИИ‑анализа по профилю и периоду, пригодный для чтения и для машинной обработки.
- Модули: Scraper (стратегии и репозитории), LLM (интеграция с моделью), Analyze (оркестрация, очереди, контроллеры/DTO, воркеры), Repositories (SQLite‑слой).

Роль человека или ИИ‑агента, который настраивает скраперы:
- Определить перечень релевантных источников по типам данных (котировки, отчеты, сделки, новости) и рынкам/тикерам.
- Оценить надежность и юридическую корректность источников (официальность, скорость обновления, условия использования/robots.txt, лимиты).
- Настроить стратегии скрапинга: форматы, селекторы, пагинацию, дедупликацию, географию/языки, частоту опроса.
- Задать нормализацию: сопоставление полей к единому виду (время в UTC, тикер/ISIN, тип события, валюта, числа), извлечение сущностей (NER) при необходимости.
- Задать параметры анализа: профили (по тикеру/сектору), временные окна (час/день/неделя), глубину выборки, temperature/системные подсказки для LLM, допустимые токены/стоимость.

Требования к качеству данных и рекомендации:
- Источники: минимум задержки обновления, устойчивые URL/эндпоинты, понятный формат (JSON/CSV/HTML/PDF), доступность исторических данных.
- Очистка/дедупликация: проверяйте хэши/идемпотентность при повторном парсинге, удаляйте дубликаты новостей с разными зеркалами.
- Временные зоны и валюты: храните в стандартизованном виде (UTC, базовая валюта), сохраняйте оригинал в метаданных.
- Юридические аспекты: соблюдайте правила использования данных, лицензии, robots.txt, и ограничители частоты запросов.

Как добавить новый источник и скрапер (общая схема):
1) Описать источник в хранилище источников (DocumentSourcesRepository) с ключевыми полями: URL/эндпоинт, тип данных, частота опроса, правила парсинга.
2) Реализовать стратегию скрапинга (в модуле Scraper) под формат источника: API/JSON, HTML/DOM, файлы (PDF/XLS). Для API используйте существующие стратегии как пример.
3) Сохранить результаты в репозитории документов (DocumentsRepository) в унифицированном формате с метаданными.
4) Привязать источник к профилям анализа (AnalysisProfilesRepository) при необходимости.
5) Запустить воркер/задачу по расписанию (BullMQ) или триггерить ручным вызовом контроллера.

Техническая среда:
- Backend: Node.js 20, NestJS, BullMQ, better-sqlite3 (SQLite). Переменные окружения: PORT, REDIS_URL (для очередей), DATABASE_PATH.
- Frontend: React, esbuild; клиент к API генерируется из Swagger/OpenAPI.
- Докер/Compose: контейнер для backend, опционально Redis.

Что на выходе получает пользователь:
- Структурированные отчеты по тикерам/профилям с краткими выводами ИИ (драйверы, риски, события, настроение новостного фона).
- Доступ к данным и отчетам через REST API и типобезопасный клиент.
- Возможность расширения новыми источниками и профилями без изменений основной архитектуры.

Кратко: это платформа для сбора и ИИ‑анализа рыночной информации. Вы добавляете источники и настраиваете скраперы — система агрегирует данные, нормализует их и генерирует понятные отчеты и сигналы для принятия решений.